{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import Model,load_model\n",
    "from keras.layers import Input, Layer,GlobalAveragePooling2D,Dense,concatenate\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import SGD,Adam\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import numpy.random as rng\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training alphabets\n",
      "dict_keys(['left', 'right'])\n",
      "validation alphabets:\n",
      "dict_keys(['left', 'right'])\n"
     ]
    }
   ],
   "source": [
    "#load datasets\n",
    "\n",
    "PATH = \"./Bongard/BP_61\" #CHANGE THIS - path where the pickled data is stored\n",
    "\n",
    "with open(os.path.join(PATH, \"train.pickle\"), \"rb\") as f:\n",
    "    (X,c) = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(PATH, \"val.pickle\"), \"rb\") as f:\n",
    "    (X_val,cval) = pickle.load(f)\n",
    "    \n",
    "print(\"training alphabets\")\n",
    "print(c.keys())\n",
    "print(\"validation alphabets:\")\n",
    "print(cval.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data from ./Bongard/BP_61/train.pickle\n",
      "loading data from ./Bongard/BP_61/val.pickle\n"
     ]
    }
   ],
   "source": [
    "class TripletBongard:\n",
    "    def __init__(self, path, data_subsets = [\"train\", \"val\"]):\n",
    "        self.data={}\n",
    "        self.categories={}\n",
    "\n",
    "        for name in data_subsets:\n",
    "            file_path= os.path.join(path, name + \".pickle\")\n",
    "            print(\"loading data from {}\".format(file_path))\n",
    "            with open(file_path,\"rb\") as f:\n",
    "                (X,c) = pickle.load(f)\n",
    "                self.data[name] = X\n",
    "                self.categories[name] = c\n",
    "\n",
    "    def triplet_generator(self, batch_size, s=\"train\"):\n",
    "\n",
    "            X=self.data[s]\n",
    "            n_classes, n_examples, w, h = X.shape\n",
    "\n",
    "            anchor_bongards=np.zeros((batch_size, w, h,3)) \n",
    "            posi_bongards=np.zeros((batch_size,  w, h,3)) \n",
    "            neg_bongards=np.zeros((batch_size,  w, h,3)) \n",
    "            store= targets=np.zeros((batch_size,5))\n",
    "            for i in range(batch_size):\n",
    "\n",
    "                rand_idx = rng.randint(0, n_classes)\n",
    "                anchor_bongard_idx = rng.randint(0, n_examples)\n",
    "                posi_bongard_idx=rng.randint(0, n_examples)\n",
    "\n",
    "                while anchor_bongard_idx == posi_bongard_idx:\n",
    "                    posi_bongard_idx = rng.randint(0, n_examples)\n",
    "\n",
    "\n",
    "                anchor_bongard = cv2.cvtColor(cv2.resize(X[rand_idx, anchor_bongard_idx], (w, h)), cv2.COLOR_GRAY2RGB)\n",
    "                posi_bongard = cv2.cvtColor(cv2.resize(X[rand_idx, posi_bongard_idx], (w, h)), cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "                neg_idx=rng.randint(0, n_classes)\n",
    "                while neg_idx==rand_idx:\n",
    "                    neg_idx = rng.randint(0, n_classes)\n",
    "\n",
    "                neg_bongard_idx=rng.randint(0, n_examples)\n",
    "                neg_bongard = cv2.cvtColor(cv2.resize(X[neg_idx, neg_bongard_idx], (w, h)), cv2.COLOR_GRAY2RGB)\n",
    "                \n",
    " \n",
    "                anchor_bongards[i,:,:,:] = anchor_bongard/255.0\n",
    "                posi_bongards[i,:,:,:] = posi_bongard/255.0\n",
    "                neg_bongards[i,:,:,:] = neg_bongard/255.0\n",
    "               \n",
    "                store[i,0]=rand_idx\n",
    "                store[i,1]= anchor_bongard_idx\n",
    "                store[i,2]= posi_bongard_idx\n",
    "                store[i,3]= neg_bongard_idx\n",
    "                store[i,4]= neg_idx\n",
    "             \n",
    "            return anchor_bongards, posi_bongards,neg_bongards,store\n",
    "        \n",
    "    \n",
    "        \n",
    "                         \n",
    "#Instantiate the class\n",
    "TripletBongard_loader= TripletBongard(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def W_init(shape,name=None):\n",
    "    \"\"\"Initialize weights as in paper\"\"\"\n",
    "    values = rng.normal(loc=0,scale=1e-2,size=shape)\n",
    "    return K.variable(values,name=name)\n",
    "#//TODO: figure out how to initialize layer biases in keras.\n",
    "def b_init(shape,name=None):\n",
    "    \"\"\"Initialize bias as in paper\"\"\"\n",
    "    values=rng.normal(loc=0.5,scale=1e-2,size=shape)\n",
    "    return K.variable(values,name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:7: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "embedding_size=4096\n",
    "input_shape = (105, 105, 3)\n",
    "vgg16_model = VGG16(weights = 'imagenet', include_top = False)\n",
    "x = vgg16_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "predictions = Dense(embedding_size,activation=\"sigmoid\",kernel_regularizer=l2(1e-3),kernel_initializer=W_init,bias_initializer=b_init)(x)\n",
    "convnet=Model(input = vgg16_model.input, output = predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, None, None, 3)     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, None, None, 64)    1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, None, None, 64)    36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, None, None, 64)    0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, None, None, 128)   73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, None, None, 128)   147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, None, None, 128)   0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, None, None, 256)   295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, None, None, 256)   0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, None, None, 512)   1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4096)              2101248   \n",
      "=================================================================\n",
      "Total params: 16,815,936\n",
      "Trainable params: 16,815,936\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "convnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input for anchor, positive and negative images\n",
    "in_a = Input(shape=(105, 105, 3))\n",
    "in_p = Input(shape=(105, 105, 3))\n",
    "in_n = Input(shape=(105, 105, 3))\n",
    "\n",
    "# Output for anchor, positive and negative embedding vectors\n",
    "emb_a = convnet(in_a)\n",
    "emb_p = convnet(in_p)\n",
    "emb_n = convnet(in_n)\n",
    "\n",
    "distance = concatenate([emb_a ,emb_p,emb_n],name='distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 105, 105, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 105, 105, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 105, 105, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_1 (Model)                 (None, 4096)         16815936    input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "distance (Concatenate)          (None, 12288)        0           model_1[1][0]                    \n",
      "                                                                 model_1[2][0]                    \n",
      "                                                                 model_1[3][0]                    \n",
      "==================================================================================================\n",
      "Total params: 16,815,936\n",
      "Trainable params: 16,815,936\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "triplet = Model(inputs=[in_a, in_p, in_n],outputs=distance)\n",
    "triplet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 105, 105, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 105, 105, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 105, 105, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_1 (Model)                 (None, 4096)         16815936    input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "distance (Concatenate)          (None, 12288)        0           model_1[1][0]                    \n",
      "                                                                 model_1[2][0]                    \n",
      "                                                                 model_1[3][0]                    \n",
      "==================================================================================================\n",
      "Total params: 16,815,936\n",
      "Trainable params: 2,101,248\n",
      "Non-trainable params: 14,714,688\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# freeze all layers of the pre-trained model\n",
    "for layer in vgg16_model.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "triplet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def triplet_loss(y_true,y_pred):\n",
    "    \"\"\"\n",
    "    Custom loss function. \n",
    "    Standard keras defined format\n",
    "    \"\"\"\n",
    "    a = y_pred[:,0:embedding_size]\n",
    "    p = y_pred[:,embedding_size:2*embedding_size]\n",
    "    n = y_pred[:,2*embedding_size:]\n",
    "    \n",
    "    p_dist = K.sum(K.square(a-p), axis=-1)\n",
    "    n_dist = K.sum(K.square(a-n), axis=-1)\n",
    "    \n",
    "    return K.sum(K.maximum(p_dist - n_dist + 0.2, 0), axis=0)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(0.00006)\n",
    "triplet.compile(optimizer = optimizer, loss = triplet_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_to_encoding(image, model):\n",
    "    w, h = image.shape\n",
    "    x=cv2.cvtColor(cv2.resize(image,(w, h)), cv2.COLOR_GRAY2RGB)\n",
    "    x1=x/255.0\n",
    "    x1 = x1[np.newaxis, :]\n",
    "    embedding = model.predict_on_batch(x1)\n",
    "    return embedding   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = {}\n",
    "database[\"left_0\"] = img_to_encoding(X[0][0], convnet)\n",
    "database[\"left_1\"] = img_to_encoding(X[0][1], convnet)\n",
    "database[\"left_2\"] = img_to_encoding(X[0][2], convnet)\n",
    "database[\"left_3\"] = img_to_encoding(X[0][3], convnet)\n",
    "database[\"left_4\"] = img_to_encoding(X[0][4], convnet)\n",
    "database[\"left_5\"] = img_to_encoding(X[0][5], convnet)\n",
    "database[\"right_0\"] = img_to_encoding(X[1][0], convnet)\n",
    "database[\"right_1\"] = img_to_encoding(X[1][1], convnet)\n",
    "database[\"right_2\"] = img_to_encoding(X[1][2], convnet)\n",
    "database[\"right_3\"] = img_to_encoding(X[1][3], convnet)\n",
    "database[\"right_4\"] = img_to_encoding(X[1][4], convnet)\n",
    "database[\"right_5\"] = img_to_encoding(X[1][5], convnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def which_is_it(image, database, model):\n",
    "    encoding = img_to_encoding(image, model)\n",
    "    min_dist = 100\n",
    "    for (name, db_enc) in database.items():\n",
    "        dist = np.linalg.norm(encoding - db_enc)\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            identity = name\n",
    "            \n",
    "    return min_dist, identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_bongard(X_val,model):\n",
    "    n_classes_val, n_examples_val, w, h = X_val.shape\n",
    "    m_val = n_classes_val * n_examples_val\n",
    "    X_val=X_val.reshape(m_val, w, h)\n",
    "    n_correct=0\n",
    "    for i in range(m_val):\n",
    "        min_dist,identity=which_is_it(X_val[i], database, model)\n",
    "        if i < m_val/2:\n",
    "            targets=0\n",
    "        else:\n",
    "            targets=1 \n",
    "        \n",
    "        if identity in [\"left_0\",\"left_1\",\"left_2\",\"left_3\",\"left_4\",\"left_5\"]:\n",
    "            test_result=0\n",
    "        else:\n",
    "            test_result=1\n",
    "            \n",
    "        if test_result== targets:\n",
    "            n_correct+=1\n",
    "         \n",
    "        #print(identity)\n",
    "    percent_correct = (100.0*n_correct / m_val)\n",
    "    print(\"Got an average of {}% accuracy\".format(percent_correct))\n",
    "     \n",
    "    return percent_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training\n",
      "0.3102381\n",
      "0.7225119\n",
      "0.94202983\n",
      "0.38728887\n",
      "0.8363625\n",
      "0.6407454\n",
      "0.9094435\n",
      "0.3811335\n",
      "0.6383462\n",
      "0.6261188\n",
      "0.8196841\n",
      "0.57960045\n",
      "0.7165394\n",
      "0.449789\n",
      "0.4910547\n",
      "0.633789\n",
      "0.7823911\n",
      "0.32462627\n",
      "0.5830572\n",
      "0.5666165\n",
      "0.50430715\n",
      "0.258701\n",
      "0.44976115\n",
      "0.47184324\n",
      "0.7464703\n",
      "0.72544736\n",
      "0.70490223\n",
      "0.1925257\n",
      "0.83427465\n",
      "0.390732\n",
      "0.19074953\n",
      "0.36410934\n",
      "0.25612515\n",
      "0.31240138\n",
      "0.65934575\n",
      "0.68835855\n",
      "0.37998888\n",
      "0.18670455\n",
      "0.39906746\n",
      "0.35700548\n",
      "0.41086346\n",
      "0.24340506\n",
      "0.8058442\n",
      "0.20111497\n",
      "0.5118183\n",
      "0.46333396\n",
      "0.49964342\n",
      "0.37342334\n",
      "0.19345541\n",
      "0.2986676\n",
      "0.49917763\n",
      "0.77486014\n",
      "0.47351402\n",
      "0.69020975\n",
      "0.45419633\n",
      "0.2434729\n",
      "0.22619568\n",
      "0.653301\n",
      "0.5862309\n",
      "0.48854318\n",
      "0.5383493\n",
      "0.5633489\n",
      "0.4298141\n",
      "0.35816866\n",
      "0.6345331\n",
      "0.5198761\n",
      "0.4898855\n",
      "0.47401285\n",
      "0.30192792\n",
      "0.47390166\n",
      "0.7800398\n",
      "0.3946751\n",
      "0.286106\n",
      "0.16944252\n",
      "0.39832938\n",
      "0.51206577\n",
      "0.16822572\n",
      "0.21092652\n",
      "0.42432737\n",
      "0.16703974\n",
      "0.3988075\n",
      "0.49308467\n",
      "0.5475849\n",
      "0.97071576\n",
      "0.26760584\n",
      "0.31896588\n",
      "0.43106955\n",
      "0.34559712\n",
      "0.54466426\n",
      "0.2749616\n",
      "0.53877044\n",
      "0.59657705\n",
      "0.38124937\n",
      "0.31573987\n",
      "0.32263976\n",
      "0.34042364\n",
      "0.33926344\n",
      "0.52750516\n",
      "0.26347014\n",
      "0.15988086\n",
      "0.5899269\n",
      "0.17910627\n",
      "0.5071763\n",
      "0.18502834\n",
      "0.26938975\n",
      "0.3823592\n",
      "0.4701931\n",
      "0.15740566\n",
      "0.39421588\n",
      "0.16648707\n",
      "0.36850625\n",
      "0.268754\n",
      "0.16487601\n",
      "0.15569177\n",
      "0.15540901\n",
      "0.23943332\n",
      "0.15483858\n",
      "0.2768697\n",
      "0.56233317\n",
      "0.37873048\n",
      "0.20857659\n",
      "0.18223153\n",
      "0.23253238\n",
      "0.3971034\n",
      "0.15266684\n",
      "0.4064788\n",
      "0.21877894\n",
      "0.29405022\n",
      "0.22932583\n",
      "0.20789008\n",
      "0.15392473\n",
      "0.17438947\n",
      "0.15414347\n",
      "0.2185371\n",
      "0.16623455\n",
      "0.24410106\n",
      "0.3157714\n",
      "0.14961568\n",
      "0.23825347\n",
      "0.14915696\n",
      "0.19688615\n",
      "0.14869635\n",
      "0.45901754\n",
      "0.22252932\n",
      "0.28901368\n",
      "0.15188912\n",
      "0.3377929\n",
      "0.28029293\n",
      "0.14711371\n",
      "0.14688641\n",
      "0.2816996\n",
      "0.14642663\n",
      "0.14619453\n",
      "0.16853866\n",
      "0.28329635\n",
      "0.14550237\n",
      "0.14527053\n",
      "0.28367132\n",
      "0.22834699\n",
      "0.1446031\n",
      "0.1443887\n",
      "0.14417022\n",
      "0.19113912\n",
      "0.16217099\n",
      "0.14350754\n",
      "0.14328466\n",
      "0.143059\n",
      "0.14283085\n",
      "0.21595795\n",
      "0.14237744\n",
      "0.14215185\n",
      "0.14192404\n",
      "0.14169426\n",
      "0.14146279\n",
      "0.14122984\n",
      "0.14099565\n",
      "0.1407604\n",
      "0.14052431\n",
      "0.14028752\n",
      "0.14005017\n",
      "0.13981242\n",
      "0.17472044\n",
      "0.13934717\n",
      "0.13911834\n",
      "0.1388887\n",
      "0.21422324\n",
      "0.13843656\n",
      "0.2677203\n",
      "0.13800575\n",
      "0.1377958\n",
      "0.13758391\n",
      "0.228601\n",
      "0.18847494\n",
      "0.13696706\n",
      "0.13676709\n",
      "0.13656467\n",
      "0.1363601\n",
      "0.13615361\n",
      "0.13594592\n",
      "0.26938778\n",
      "0.15435068\n",
      "0.2124713\n",
      "0.13516654\n",
      "0.13498053\n",
      "0.15335453\n",
      "0.13460897\n",
      "0.13442329\n",
      "0.13423479\n",
      "0.21522021\n",
      "0.13385959\n",
      "0.14014596\n",
      "0.13349184\n",
      "0.22825152\n",
      "0.13313124\n",
      "0.177098\n",
      "0.13277575\n",
      "0.13259731\n",
      "0.13241613\n",
      "0.1322325\n",
      "0.13204667\n",
      "0.13185892\n",
      "0.15877193\n",
      "0.15262452\n",
      "0.15496387\n",
      "0.20041366\n",
      "0.13097002\n",
      "0.13080102\n",
      "0.2056956\n",
      "0.13046443\n",
      "0.1302965\n",
      "0.13012569\n",
      "0.12995233\n",
      "0.12977667\n",
      "0.129599\n",
      "0.12941952\n",
      "0.12989083\n",
      "0.1290671\n",
      "0.1288935\n",
      "0.128718\n",
      "0.18896514\n",
      "0.12837568\n",
      "0.12820803\n",
      "0.12803811\n",
      "0.1278662\n",
      "0.12769252\n",
      "0.12751731\n",
      "0.12734075\n",
      "0.12716304\n",
      "0.1269843\n",
      "0.12680472\n",
      "0.15743154\n",
      "0.15603837\n",
      "0.12630281\n",
      "0.12614454\n",
      "0.12598386\n",
      "0.12582101\n",
      "0.12565623\n",
      "0.15862255\n",
      "0.12533364\n",
      "0.1251752\n",
      "0.12501465\n",
      "0.124852255\n",
      "0.124688216\n",
      "0.12452271\n",
      "0.124355935\n",
      "0.12418804\n",
      "0.1240192\n",
      "0.15994349\n",
      "0.21485874\n",
      "0.123551145\n",
      "0.12340348\n",
      "0.12325332\n",
      "0.12310091\n",
      "0.12294648\n",
      "0.12279026\n",
      "0.12263246\n",
      "0.12247325\n",
      "0.14206105\n",
      "0.122163944\n",
      "0.13674498\n",
      "0.12187305\n",
      "0.12173042\n",
      "0.12158538\n",
      "0.12143815\n",
      "0.12128898\n",
      "0.121138036\n",
      "0.12098553\n",
      "0.12083162\n",
      "0.13350977\n",
      "0.1205342\n",
      "0.12038967\n",
      "0.12024318\n",
      "0.1200949\n",
      "0.119945064\n",
      "0.11979385\n",
      "0.11964142\n",
      "0.119487934\n",
      "0.1319268\n",
      "0.11919255\n",
      "0.11904964\n",
      "0.118904985\n",
      "0.1187588\n",
      "0.118611224\n",
      "0.11846243\n",
      "0.1183126\n",
      "0.1181618\n",
      "0.11801022\n",
      "0.11785795\n",
      "0.117705084\n",
      "0.11755174\n",
      "0.117397964\n",
      "0.11724387\n",
      "0.11708952\n",
      "0.11693497\n",
      "0.11678028\n",
      "0.11662552\n",
      "0.11647071\n",
      "0.116315894\n",
      "0.11616115\n",
      "0.116006464\n",
      "0.115851894\n",
      "0.11569745\n",
      "0.115543164\n",
      "0.11538908\n",
      "0.11523518\n",
      "0.11508151\n",
      "0.114928074\n",
      "0.11477489\n",
      "0.11462197\n",
      "0.11446932\n",
      "0.11431696\n",
      "0.114164874\n",
      "0.114013314\n",
      "0.113861844\n",
      "0.11371069\n",
      "0.11355986\n",
      "0.11340952\n",
      "0.11325934\n",
      "0.11310948\n",
      "0.11295995\n",
      "0.112810746\n",
      "0.112661876\n",
      "0.11251335\n",
      "0.11236517\n",
      "0.11221732\n",
      "0.112069815\n",
      "0.111922644\n",
      "0.111775815\n",
      "0.11162931\n",
      "0.111483164\n",
      "0.13896367\n",
      "0.11119989\n",
      "0.111062065\n",
      "0.110924035\n",
      "0.11078583\n",
      "0.110647455\n",
      "0.11050895\n",
      "0.11037035\n",
      "0.11023168\n",
      "0.11009297\n",
      "0.10995423\n",
      "0.10981549\n",
      "0.10967679\n",
      "0.109538145\n",
      "0.109399565\n",
      "0.10926109\n",
      "0.109122716\n",
      "0.10898448\n",
      "0.10884638\n",
      "0.108708456\n",
      "0.10857068\n",
      "0.1084331\n",
      "0.10829571\n",
      "0.108158514\n",
      "0.10802154\n",
      "0.10788479\n",
      "0.10774825\n",
      "0.107611954\n",
      "0.1074759\n",
      "0.10734007\n",
      "0.10720448\n",
      "0.12862325\n",
      "0.106948555\n",
      "0.10682689\n",
      "0.106704265\n",
      "0.10658078\n",
      "0.106456555\n",
      "0.10633164\n",
      "0.10620616\n",
      "0.106080174\n",
      "0.10595374\n",
      "0.10582694\n",
      "0.10569984\n",
      "0.10557246\n",
      "0.10544487\n",
      "0.10531713\n",
      "0.105189264\n",
      "0.10506129\n",
      "0.104933284\n",
      "0.104805246\n",
      "0.10467721\n",
      "0.10454919\n",
      "0.104421236\n",
      "0.10429333\n",
      "0.10416553\n",
      "0.10403782\n",
      "0.10391024\n",
      "0.10378277\n",
      "0.10365546\n",
      "0.103528306\n",
      "0.1034013\n",
      "0.10327445\n",
      "0.103147805\n",
      "0.10302133\n",
      "0.10289505\n",
      "0.102768965\n",
      "0.10264307\n",
      "0.10251738\n",
      "0.1023919\n",
      "0.102266625\n",
      "0.10214156\n",
      "0.1020167\n",
      "0.10189208\n",
      "0.10176767\n",
      "0.101643465\n",
      "0.101519495\n",
      "0.10139573\n",
      "0.10127219\n",
      "0.10114888\n",
      "0.10102579\n",
      "0.10090293\n",
      "0.100780286\n",
      "0.10065786\n",
      "0.10053566\n",
      "0.10041368\n",
      "0.10029193\n",
      "0.100170396\n",
      "0.10004909\n",
      "0.09992799\n",
      "0.09980713\n",
      "0.09968647\n",
      "0.09956605\n",
      "0.099445835\n",
      "0.09932585\n",
      "0.099206075\n",
      "0.099086516\n",
      "0.098967165\n",
      "0.098848045\n",
      "0.09872913\n",
      "0.09861044\n",
      "0.09849195\n",
      "0.09837368\n",
      "0.09825563\n",
      "0.09813778\n",
      "0.09802014\n",
      "0.09790271\n",
      "0.09778548\n",
      "0.09766848\n",
      "0.09755167\n",
      "0.09743507\n",
      "0.09731867\n",
      "0.097202495\n",
      "0.097086504\n",
      "0.09697072\n",
      "0.096855134\n",
      "0.096739754\n",
      "0.096624576\n",
      "0.09650959\n",
      "0.096394815\n",
      "0.096280225\n",
      "0.096165836\n",
      "0.096051656\n",
      "0.095937654\n",
      "0.09582386\n",
      "0.09571025\n",
      "0.095596835\n",
      "0.095483616\n",
      "0.09537059\n",
      "0.09525777\n",
      "0.095145114\n",
      "0.09503266\n",
      "0.0949204\n",
      "0.09480832\n",
      "0.09469642\n",
      "0.09458472\n",
      "0.0944732\n",
      "0.09436186\n",
      "0.09425071\n",
      "0.09413975\n",
      "0.094028965\n",
      "0.09391837\n",
      "0.09380795\n",
      "0.09369773\n",
      "0.09358766\n",
      "0.093477786\n",
      "0.09336808\n",
      "0.09325857\n",
      "0.09314922\n",
      "0.093040064\n",
      "0.09293107\n",
      "0.09282226\n",
      "0.092713624\n",
      "0.09260515\n",
      "0.092496864\n",
      "0.09238875\n",
      "0.0922808\n",
      "0.09217304\n",
      "0.092065424\n",
      "0.091958\n",
      "0.09185074\n",
      "0.091743656\n",
      "0.091636725\n",
      "0.09415084\n",
      "0.09144585\n",
      "0.09135989\n",
      "0.091272265\n",
      "0.091183126\n",
      "0.09109259\n",
      "0.09543324\n",
      "0.09092181\n",
      "0.09084055\n",
      "0.09075721\n",
      "0.090671964\n",
      "0.09058501\n",
      "0.090496495\n",
      "0.095986746\n",
      "0.090327196\n",
      "0.09024582\n",
      "0.090162605\n",
      "0.09007768\n",
      "0.08999121\n",
      "0.089903325\n",
      "0.08981414\n",
      "0.08972378\n",
      "0.08963237\n",
      "0.089540005\n",
      "0.08944677\n",
      "0.08935276\n",
      "0.08925809\n",
      "0.08916278\n",
      "0.089066945\n",
      "0.08897063\n",
      "0.0888739\n",
      "0.0887768\n",
      "0.08867939\n",
      "0.088581696\n",
      "0.08848377\n",
      "0.08838566\n",
      "0.08828738\n",
      "0.08818895\n",
      "0.08809045\n",
      "0.08799184\n",
      "0.08789317\n",
      "0.08779446\n",
      "0.08769574\n",
      "0.087597\n",
      "0.08749828\n",
      "0.08739956\n",
      "0.08730089\n",
      "0.08720225\n",
      "0.08710368\n",
      "0.08700517\n",
      "0.08690672\n",
      "0.086808346\n",
      "0.086710066\n",
      "0.08661187\n",
      "0.08651376\n",
      "0.08641574\n",
      "0.086317845\n",
      "0.115971975\n",
      "0.08613888\n",
      "0.08605639\n",
      "0.08597265\n",
      "0.0858878\n",
      "0.08580191\n",
      "0.085715085\n",
      "0.08562741\n",
      "0.08553898\n",
      "0.085449874\n",
      "0.08536014\n",
      "0.085269876\n",
      "0.08517913\n",
      "0.08508795\n",
      "0.08499639\n",
      "0.0849045\n",
      "0.084812336\n",
      "0.08471992\n",
      "0.084627286\n",
      "0.08453449\n",
      "0.08444153\n",
      "0.084348455\n",
      "0.08425528\n",
      "0.08416202\n",
      "0.08406871\n",
      "0.08397536\n",
      "0.083882\n",
      "0.08378862\n",
      "0.08369525\n",
      "0.08360189\n",
      "0.083508566\n",
      "0.08341527\n",
      "0.08332208\n",
      "0.083228886\n",
      "0.08313576\n",
      "0.083042696\n",
      "0.0829497\n",
      "0.08285678\n",
      "0.082763955\n",
      "0.08267122\n",
      "0.082578555\n",
      "0.082485996\n",
      "0.08239353\n",
      "0.082301155\n",
      "0.08220889\n",
      "0.08211673\n",
      "0.08202466\n",
      "0.0819327\n",
      "0.08184086\n",
      "0.08174912\n",
      "0.081657484\n",
      "0.08156597\n",
      "0.08147456\n",
      "0.08138327\n",
      "0.081292085\n",
      "0.08120102\n",
      "0.08111007\n",
      "0.081019215\n",
      "0.08092849\n",
      "0.080837876\n",
      "0.08074738\n",
      "0.080657005\n",
      "0.08056672\n",
      "0.080476575\n",
      "0.080386534\n",
      "0.080296606\n",
      "0.080206804\n",
      "0.080117114\n",
      "0.08002753\n",
      "0.07993812\n",
      "0.07984877\n",
      "0.07975953\n",
      "0.0796704\n",
      "0.0795814\n",
      "0.07949251\n",
      "0.07940373\n",
      "0.07931507\n",
      "0.07922651\n",
      "0.07913807\n",
      "0.07904974\n",
      "0.07896152\n",
      "0.07887343\n",
      "0.07878544\n",
      "0.07869756\n",
      "0.078609794\n",
      "0.078522146\n",
      "0.0784346\n",
      "0.07834718\n",
      "0.078259856\n",
      "0.07817264\n",
      "0.07808554\n",
      "0.077998556\n",
      "0.077911675\n",
      "0.07782491\n",
      "0.07773825\n",
      "0.0776517\n",
      "0.07756525\n",
      "0.077478915\n",
      "0.0773927\n",
      "0.077306576\n",
      "0.077220574\n",
      "0.07713467\n",
      "0.07704888\n",
      "0.07696319\n",
      "0.07687762\n",
      "0.08266502\n",
      "0.076725945\n",
      "0.07665814\n",
      "0.07658883\n",
      "0.07651818\n",
      "0.07644628\n",
      "0.07637325\n",
      "0.076299205\n",
      "0.076224245\n",
      "0.07614844\n",
      "0.07607191\n",
      "0.0759947\n",
      "0.08150835\n",
      "0.075856835\n",
      "0.075794674\n",
      "0.07573058\n",
      "0.07566474\n",
      "0.075597286\n",
      "0.075528376\n",
      "0.07545815\n",
      "0.07538673\n",
      "0.07531424\n",
      "0.07524077\n",
      "0.07516644\n",
      "0.075091325\n",
      "0.075015515\n",
      "0.07493907\n",
      "0.074862085\n",
      "0.07478461\n",
      "0.07470668\n",
      "0.07462837\n",
      "0.07454973\n",
      "0.07447079\n",
      "0.07439159\n",
      "0.074312165\n",
      "0.074232556\n",
      "0.07415278\n",
      "0.07407286\n",
      "0.07399283\n",
      "0.07391271\n",
      "0.073832504\n",
      "0.073752254\n",
      "0.07367194\n",
      "0.07359162\n",
      "0.07351127\n",
      "0.07343092\n",
      "0.07335057\n",
      "0.07327025\n",
      "0.07318992\n",
      "0.07310965\n",
      "0.0730294\n",
      "0.0729492\n",
      "0.07286903\n",
      "0.07278893\n",
      "0.07270889\n",
      "0.0726289\n",
      "0.073201254\n",
      "0.07248735\n",
      "0.07242429\n",
      "0.072359905\n",
      "0.07229427\n",
      "0.0722275\n",
      "0.07215966\n",
      "0.07209086\n",
      "0.072021164\n",
      "0.07195066\n",
      "0.07187943\n",
      "0.07180755\n",
      "0.071735054\n",
      "0.07166202\n",
      "0.07158852\n",
      "0.0715146\n",
      "0.07144028\n",
      "0.07136563\n",
      "0.07129069\n",
      "0.07121548\n",
      "0.07114004\n",
      "0.07106441\n",
      "0.07098861\n",
      "0.07091267\n",
      "0.070836596\n",
      "0.07076043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07068418\n",
      "0.070607856\n",
      "0.07053148\n",
      "0.070455074\n",
      "0.070378646\n",
      "0.070302196\n",
      "0.070225745\n",
      "0.0701493\n",
      "0.070072874\n",
      "0.06999646\n",
      "0.06992008\n",
      "0.069843724\n",
      "0.069767416\n",
      "0.06969116\n",
      "0.06961493\n",
      "0.06953877\n",
      "0.069462664\n",
      "0.069386624\n",
      "0.069310635\n",
      "0.06923471\n",
      "0.06915885\n",
      "0.069083065\n",
      "0.06900735\n",
      "0.068931706\n",
      "0.068856135\n",
      "0.06878063\n",
      "0.068705216\n",
      "0.06862987\n",
      "0.06855461\n",
      "0.06847942\n",
      "0.075417206\n",
      "0.06834942\n",
      "0.06829284\n",
      "0.06823468\n",
      "0.06817508\n",
      "0.06811416\n",
      "0.06805202\n",
      "0.067988805\n",
      "0.067924574\n",
      "0.06785945\n",
      "0.06779351\n",
      "0.067726836\n",
      "0.0676595\n",
      "0.06759157\n",
      "0.06752312\n",
      "0.06745419\n",
      "0.06738483\n",
      "0.06731511\n",
      "0.06724506\n",
      "0.067174725\n",
      "0.067104116\n",
      "0.067033306\n",
      "0.06696229\n",
      "0.06689111\n",
      "0.066819794\n",
      "0.06674836\n",
      "0.06667681\n",
      "0.06660519\n",
      "0.06653351\n",
      "0.066461764\n",
      "0.06638999\n",
      "0.06631819\n",
      "0.06624637\n",
      "0.066174544\n",
      "0.06610271\n",
      "0.06603091\n",
      "0.06595912\n",
      "0.06588735\n",
      "0.065815605\n",
      "0.06851842\n",
      "0.065691054\n",
      "0.065636605\n",
      "0.065580666\n",
      "0.06552337\n",
      "0.065464824\n",
      "0.06540513\n",
      "0.06534439\n",
      "0.06528272\n",
      "0.06522017\n",
      "0.06515685\n",
      "0.06509282\n",
      "0.06502818\n",
      "0.06496297\n",
      "0.06489725\n",
      "0.06483108\n",
      "0.06476451\n",
      "0.06469757\n",
      "0.06463032\n",
      "0.064562805\n",
      "0.06449503\n",
      "0.06589521\n",
      "0.064380206\n",
      "0.06433139\n",
      "0.06428073\n",
      "0.064228386\n",
      "0.06417448\n",
      "0.064119145\n",
      "0.0640625\n",
      "0.06400466\n",
      "0.06394574\n",
      "0.06388583\n",
      "0.06601345\n",
      "0.06377988\n",
      "0.06373259\n",
      "0.06368333\n",
      "0.06363225\n",
      "0.06357952\n",
      "0.06352528\n",
      "0.06346966\n",
      "0.0634128\n",
      "0.063354805\n",
      "0.0632958\n",
      "0.06323585\n",
      "0.06317508\n",
      "0.06311357\n",
      "0.06305139\n",
      "0.0629886\n",
      "0.06292527\n",
      "0.06286147\n",
      "0.06279725\n",
      "0.06273264\n",
      "0.06266771\n",
      "0.06260248\n",
      "0.062537\n",
      "0.062471293\n",
      "0.06240539\n",
      "0.062339313\n",
      "0.062273093\n",
      "0.06220676\n",
      "0.062140305\n",
      "0.062073775\n",
      "0.062007174\n",
      "0.06194053\n",
      "0.061873827\n",
      "0.061807103\n",
      "0.061740357\n",
      "0.061673597\n",
      "0.06160684\n",
      "0.061540086\n",
      "0.06147335\n",
      "0.061406635\n",
      "0.061339933\n",
      "0.06127327\n",
      "0.061206635\n",
      "0.061140038\n",
      "0.061073486\n",
      "0.061006986\n",
      "0.060940526\n",
      "0.06087412\n",
      "0.060807772\n",
      "0.06074147\n",
      "0.060675226\n",
      "0.060609043\n",
      "0.06054292\n",
      "0.060476862\n",
      "0.060410865\n",
      "0.060344927\n",
      "0.06027906\n",
      "0.06146264\n",
      "0.060169887\n",
      "0.060124688\n",
      "0.06007778\n",
      "0.06002929\n",
      "0.059979342\n",
      "0.059928037\n",
      "0.059875503\n",
      "0.059821855\n",
      "0.059767164\n",
      "0.05971154\n",
      "0.05965507\n",
      "0.059597835\n",
      "0.059539907\n",
      "0.05948136\n",
      "0.059422247\n",
      "0.059362642\n",
      "0.059302587\n",
      "0.05924213\n",
      "0.05918132\n",
      "0.0591202\n",
      "0.059058804\n",
      "0.058997154\n",
      "0.0589353\n",
      "0.058873255\n",
      "0.058811054\n",
      "0.058748707\n",
      "0.05868625\n",
      "0.058623686\n",
      "0.058561042\n",
      "0.058498327\n",
      "0.05843556\n",
      "0.05837275\n",
      "0.058309913\n",
      "0.060700245\n",
      "0.05819418\n",
      "0.058140863\n",
      "0.05808705\n",
      "0.058032714\n",
      "0.057977837\n",
      "0.057922438\n",
      "0.057866517\n",
      "0.05781011\n",
      "0.057753228\n",
      "0.0576959\n",
      "0.05763817\n",
      "0.057580054\n",
      "0.057521604\n",
      "0.057462826\n",
      "0.057403766\n",
      "0.05734444\n",
      "0.05728489\n",
      "0.05722513\n",
      "0.05716519\n",
      "0.057105087\n",
      "0.05704484\n",
      "0.05698447\n",
      "0.056924\n",
      "0.056863442\n",
      "0.05680281\n",
      "0.056742113\n",
      "0.05668137\n",
      "0.056620583\n",
      "0.05655978\n",
      "0.056498952\n",
      "0.05643812\n",
      "0.056377277\n",
      "0.056316443\n",
      "0.056255616\n",
      "0.056194812\n",
      "0.05613402\n",
      "0.05607326\n",
      "0.05601253\n",
      "0.055951826\n",
      "0.055891164\n",
      "0.055830546\n",
      "0.055769965\n",
      "0.05570943\n",
      "0.05564894\n",
      "0.0555885\n",
      "0.05552811\n",
      "0.055467777\n",
      "0.055407487\n",
      "0.055347264\n",
      "0.05528709\n"
     ]
    }
   ],
   "source": [
    "batch_size=2\n",
    "n_iter=1000\n",
    "y = [i for i in range(batch_size)]\n",
    "qy = np.array(y)\n",
    "\n",
    "print(\"training\")\n",
    "for i in range(1, n_iter):\n",
    "    a,p,n,store= TripletBongard_loader.triplet_generator(batch_size=2)\n",
    "    loss=triplet.train_on_batch([a,p,n], qy)\n",
    "    print(loss)\n",
    "   # if i > n_iter/2:\n",
    "        #print(\"evaluating\")\n",
    "       # val_acc = test_bongard(X_val,convnet)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "convnet.save('bottleneck_fc_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got an average of 54.2% accuracy\n"
     ]
    }
   ],
   "source": [
    "val_acc = test_bongard(X_val,convnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, None, None, 3)     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, None, None, 64)    1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, None, None, 64)    36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, None, None, 64)    0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, None, None, 128)   73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, None, None, 128)   147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, None, None, 128)   0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, None, None, 256)   295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, None, None, 256)   0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, None, None, 512)   1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4096)              2101248   \n",
      "=================================================================\n",
      "Total params: 16,815,936\n",
      "Trainable params: 9,180,672\n",
      "Non-trainable params: 7,635,264\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for layer in convnet.layers[15:]:\n",
    "    layer.trainable = True\n",
    "convnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input for anchor, positive and negative images\n",
    "in_a_1 = Input(shape=(105, 105, 3))\n",
    "in_p_1 = Input(shape=(105, 105, 3))\n",
    "in_n_1 = Input(shape=(105, 105, 3))\n",
    "\n",
    "# Output for anchor, positive and negative embedding vectors\n",
    "emb_a_1 = convnet(in_a_1)\n",
    "emb_p_1 = convnet(in_p_1)\n",
    "emb_n_1 = convnet(in_n_1)\n",
    "\n",
    "distance_1 = concatenate([emb_a_1 ,emb_p_1,emb_n_1],name='distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 105, 105, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_12 (InputLayer)           (None, 105, 105, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_13 (InputLayer)           (None, 105, 105, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_1 (Model)                 (None, 4096)         16815936    input_11[0][0]                   \n",
      "                                                                 input_12[0][0]                   \n",
      "                                                                 input_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "distance (Concatenate)          (None, 12288)        0           model_1[10][0]                   \n",
      "                                                                 model_1[11][0]                   \n",
      "                                                                 model_1[12][0]                   \n",
      "==================================================================================================\n",
      "Total params: 16,815,936\n",
      "Trainable params: 9,180,672\n",
      "Non-trainable params: 7,635,264\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "triplet_1 = Model(inputs=[in_a_1, in_p_1, in_n_1],outputs=distance_1)\n",
    "triplet_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = SGD(lr=1e-4, momentum=0.9)\n",
    "triplet_1.compile(optimizer = optimizer, loss = triplet_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training\n",
      "0.055226974\n",
      "0.055166915\n",
      "0.055106916\n",
      "0.055046972\n",
      "0.054987106\n",
      "0.05492729\n",
      "0.054867525\n",
      "0.054807827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/engine/training.py:975: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05474819\n",
      "0.05468861\n",
      "0.054629102\n",
      "0.054569647\n",
      "0.05451026\n",
      "0.05445093\n",
      "0.054391664\n",
      "0.05433246\n",
      "0.054273322\n",
      "0.059081264\n",
      "0.0541778\n",
      "0.054139547\n",
      "0.05409961\n",
      "0.054058097\n",
      "0.054015115\n",
      "0.05397078\n",
      "0.053925198\n",
      "0.053878475\n",
      "0.053830706\n",
      "0.053781983\n",
      "0.053732395\n",
      "0.053682026\n",
      "0.053630933\n",
      "0.057338215\n",
      "0.05354103\n",
      "0.05350169\n",
      "0.053461164\n",
      "0.05341946\n",
      "0.0533766\n",
      "0.05333262\n",
      "0.053287566\n",
      "0.05324152\n",
      "0.05319453\n",
      "0.05314665\n",
      "0.053097963\n",
      "0.053048525\n",
      "0.052998394\n",
      "0.05294764\n",
      "0.052896313\n",
      "0.052844465\n",
      "0.05279214\n",
      "0.052739393\n",
      "0.052686267\n",
      "0.0526328\n",
      "0.05257903\n",
      "0.052524984\n",
      "0.052470703\n",
      "0.052416205\n",
      "0.052361526\n",
      "0.05230668\n",
      "0.052251697\n",
      "0.052196592\n",
      "0.052141383\n",
      "0.052086093\n",
      "0.052030716\n",
      "0.051975287\n",
      "0.05191981\n",
      "0.051864296\n",
      "0.05180876\n",
      "0.0517532\n",
      "0.051697634\n",
      "0.05164206\n",
      "0.05158649\n",
      "0.06328373\n",
      "0.051495284\n",
      "0.05145804\n",
      "0.05141928\n",
      "0.051379114\n",
      "0.05133763\n",
      "0.051294938\n",
      "0.05125112\n",
      "0.051206276\n",
      "0.051160485\n",
      "0.051113836\n",
      "0.051066406\n",
      "0.051018268\n",
      "0.05096948\n",
      "0.05092012\n",
      "0.05087023\n",
      "0.050819878\n",
      "0.050769106\n",
      "0.050717942\n",
      "0.05066646\n",
      "0.050614674\n",
      "0.05056263\n",
      "0.05051035\n",
      "0.05045786\n",
      "0.05502324\n",
      "0.050370477\n",
      "0.050334208\n",
      "0.050296478\n",
      "0.050257366\n",
      "0.050216984\n",
      "0.050175395\n",
      "0.050132703\n",
      "0.05008901\n",
      "0.050044373\n",
      "0.049998887\n",
      "0.04995263\n",
      "0.04990566\n",
      "0.049858056\n",
      "0.04980988\n",
      "0.049761172\n",
      "0.049712002\n",
      "0.049662415\n",
      "0.049612448\n",
      "0.049562145\n",
      "0.049511548\n",
      "0.04946068\n",
      "0.049409583\n",
      "0.04935827\n",
      "0.049306784\n",
      "0.049255144\n",
      "0.04920336\n",
      "0.049151465\n",
      "0.049099468\n",
      "0.04904739\n",
      "0.048995234\n",
      "0.04894303\n",
      "0.04889077\n",
      "0.048838474\n",
      "0.04878616\n",
      "0.048733823\n",
      "0.04868147\n",
      "0.04862912\n",
      "0.048576772\n",
      "0.048524424\n",
      "0.0484721\n",
      "0.04841979\n",
      "0.048367497\n",
      "0.04831523\n",
      "0.048262995\n",
      "0.048210785\n",
      "0.048158616\n",
      "0.048106477\n",
      "0.04805438\n",
      "0.048002318\n",
      "0.047950298\n",
      "0.047898322\n",
      "0.04784639\n",
      "0.047794506\n",
      "0.047742665\n",
      "0.047690872\n",
      "0.047639135\n",
      "0.047587436\n",
      "0.047535792\n",
      "0.04855164\n",
      "0.04745494\n",
      "0.047423862\n",
      "0.047391072\n",
      "0.047356702\n",
      "0.04732085\n",
      "0.047283642\n",
      "0.047245186\n",
      "0.04720558\n",
      "0.047164936\n",
      "0.04712334\n",
      "0.047080874\n",
      "0.047037616\n",
      "0.04699366\n",
      "0.06388697\n",
      "0.04692577\n",
      "0.046900123\n",
      "0.0468723\n",
      "0.046842452\n",
      "0.04681073\n",
      "0.046777297\n",
      "0.046742287\n",
      "0.04670584\n",
      "0.046668075\n",
      "0.046629116\n",
      "0.046589077\n",
      "0.046548046\n",
      "0.046506137\n",
      "0.046463422\n",
      "0.046419982\n",
      "0.046375882\n",
      "0.0463312\n",
      "0.04628599\n",
      "0.04624031\n",
      "0.04619421\n",
      "0.04614774\n",
      "0.04610093\n",
      "0.046053823\n",
      "0.046006456\n",
      "0.04595886\n",
      "0.045911055\n",
      "0.045863077\n",
      "0.045814946\n",
      "0.045766674\n",
      "0.04571829\n",
      "0.0456698\n",
      "0.04562123\n",
      "0.045572598\n",
      "0.045523897\n",
      "0.04547515\n",
      "0.045426372\n",
      "0.045377567\n",
      "0.045328736\n",
      "0.045279894\n",
      "0.045231048\n",
      "0.045182202\n",
      "0.04513336\n",
      "0.04508453\n",
      "0.045035712\n",
      "0.044986915\n",
      "0.044938136\n",
      "0.04488939\n",
      "0.044840664\n",
      "0.04479197\n",
      "0.044743314\n",
      "0.04469469\n",
      "0.044646095\n",
      "0.044597544\n",
      "0.04454903\n",
      "0.04450056\n",
      "0.04445213\n",
      "0.044403743\n",
      "0.0443554\n",
      "0.0443071\n",
      "0.04425885\n",
      "0.044210643\n",
      "0.044162482\n",
      "0.044114374\n",
      "0.04406631\n",
      "0.04401829\n",
      "0.043970317\n",
      "0.0439224\n",
      "0.04387453\n",
      "0.043826703\n",
      "0.04377893\n",
      "0.04373121\n",
      "0.04368353\n",
      "0.0436359\n",
      "0.043588333\n",
      "0.043540806\n",
      "0.04349333\n",
      "0.043445904\n",
      "0.04339853\n",
      "0.043351203\n",
      "0.04342701\n",
      "0.04327432\n",
      "0.0432436\n",
      "0.043211766\n",
      "0.04317885\n",
      "0.043144878\n",
      "0.043109905\n",
      "0.043073975\n",
      "0.04303716\n",
      "0.042999513\n",
      "0.04296109\n",
      "0.04292196\n",
      "0.04288217\n",
      "0.042841785\n",
      "0.04280085\n",
      "0.04275941\n",
      "0.042717528\n",
      "0.04267524\n",
      "0.04431279\n",
      "0.042605318\n",
      "0.042576868\n",
      "0.04254722\n",
      "0.04251639\n",
      "0.04248442\n",
      "0.04245136\n",
      "0.042417265\n",
      "0.0423822\n",
      "0.042346224\n",
      "0.042309403\n",
      "0.04736025\n",
      "0.042256884\n",
      "0.04223931\n",
      "0.04221927\n",
      "0.042196937\n",
      "0.042172503\n",
      "0.042146128\n",
      "0.042117983\n",
      "0.0420882\n",
      "0.042056948\n",
      "0.042024337\n",
      "0.041990507\n",
      "0.041955568\n",
      "0.04191962\n",
      "0.041882757\n",
      "0.04184508\n",
      "0.04180666\n",
      "0.04176758\n",
      "0.0417279\n",
      "0.041687682\n",
      "0.041646983\n",
      "0.04160586\n",
      "0.041564353\n",
      "0.043265034\n",
      "0.041503467\n",
      "0.047701355\n",
      "0.041482423\n",
      "0.041479852\n",
      "0.041474525\n",
      "0.041466445\n",
      "0.041455667\n",
      "0.04144226\n",
      "0.041426346\n",
      "0.041408047\n",
      "0.041387513\n",
      "0.041364875\n",
      "0.041340284\n",
      "0.041313887\n",
      "0.041285817\n",
      "0.04125622\n",
      "0.041225214\n",
      "0.041192915\n",
      "0.042565424\n",
      "0.041139416\n",
      "0.041117657\n",
      "0.041094214\n",
      "0.041069146\n",
      "0.041042525\n",
      "0.04101443\n",
      "0.04098496\n",
      "0.04095419\n",
      "0.040922236\n",
      "0.04088917\n",
      "0.040855095\n",
      "0.040820085\n",
      "0.04078422\n",
      "0.040747587\n",
      "0.040710244\n",
      "0.040672265\n",
      "0.04063371\n",
      "0.040594637\n",
      "0.040555093\n",
      "0.04051513\n",
      "0.04047479\n",
      "0.04043412\n",
      "0.040393144\n",
      "0.040351916\n",
      "0.040310442\n",
      "0.04026877\n",
      "0.040226914\n",
      "0.040184896\n",
      "0.040142745\n",
      "0.040100478\n",
      "0.040058102\n",
      "0.040015638\n",
      "0.048839968\n",
      "0.039950144\n",
      "0.03992576\n",
      "0.039900005\n",
      "0.0398729\n",
      "0.03984452\n",
      "0.039814938\n",
      "0.039784223\n",
      "0.039752454\n",
      "0.0397197\n",
      "0.039686047\n",
      "0.03965156\n",
      "0.039616313\n",
      "0.039580364\n",
      "0.039543785\n",
      "0.039506625\n",
      "0.039468948\n",
      "0.039430797\n",
      "0.03939222\n",
      "0.039353266\n",
      "0.03931397\n",
      "0.03927436\n",
      "0.03923448\n",
      "0.039194364\n",
      "0.03915403\n",
      "0.03911351\n",
      "0.039072815\n",
      "0.039031982\n",
      "0.03899102\n",
      "0.04363715\n",
      "0.038931403\n",
      "0.038910933\n",
      "0.03888865\n",
      "0.03886467\n",
      "0.038839113\n",
      "0.03881209\n",
      "0.03878372\n",
      "0.038754113\n",
      "0.038723376\n",
      "0.03869159\n",
      "0.038658865\n",
      "0.03862528\n",
      "0.03859092\n",
      "0.03855585\n",
      "0.03852014\n",
      "0.038483854\n",
      "0.038447056\n",
      "0.038409792\n",
      "0.03837211\n",
      "0.038334053\n",
      "0.038295664\n",
      "0.038256973\n",
      "0.038218025\n",
      "0.038178846\n",
      "0.03813946\n",
      "0.038099885\n",
      "0.038060162\n",
      "0.038020294\n",
      "0.037980303\n",
      "0.03794022\n",
      "0.037900034\n",
      "0.037859783\n",
      "0.03781946\n",
      "0.03777909\n",
      "0.03773868\n",
      "0.03769823\n",
      "0.037657764\n",
      "0.037617274\n",
      "0.037576772\n",
      "0.037536263\n",
      "0.037495755\n",
      "0.03745525\n",
      "0.037414752\n",
      "0.037374273\n",
      "0.037333805\n",
      "0.037293356\n",
      "0.037252925\n",
      "0.03721252\n",
      "0.03717215\n",
      "0.037131794\n",
      "0.037091475\n",
      "0.037051186\n",
      "0.037010927\n",
      "0.036970705\n",
      "0.036930516\n",
      "0.036890365\n",
      "0.036850248\n",
      "0.036810167\n",
      "0.036770128\n",
      "0.036730126\n",
      "0.036690164\n",
      "0.03665024\n",
      "0.03661036\n",
      "0.03657052\n",
      "0.03653072\n",
      "0.036490962\n",
      "0.036451247\n",
      "0.03641157\n",
      "0.03637194\n",
      "0.036332343\n",
      "0.0362928\n",
      "0.03625329\n",
      "0.036213823\n",
      "0.036174398\n",
      "0.036135018\n",
      "0.036095683\n",
      "0.03605639\n",
      "0.036017135\n",
      "0.035977922\n",
      "0.035938755\n",
      "0.03589963\n",
      "0.03586055\n",
      "0.03582151\n",
      "0.035782512\n",
      "0.035743557\n",
      "0.035704646\n",
      "0.035665773\n",
      "0.035626944\n",
      "0.035588156\n",
      "0.03554941\n",
      "0.03551071\n",
      "0.035472054\n",
      "0.035433434\n",
      "0.03539486\n",
      "0.035356324\n",
      "0.03531783\n",
      "0.03527938\n",
      "0.03524097\n",
      "0.0352026\n",
      "0.035164278\n",
      "0.03512599\n",
      "0.035087746\n",
      "0.035049543\n",
      "0.03501138\n",
      "0.034973264\n",
      "0.034935184\n",
      "0.03489715\n",
      "0.034859147\n",
      "0.034821194\n",
      "0.034783274\n",
      "0.034745403\n",
      "0.03470757\n",
      "0.034669776\n",
      "0.034632023\n",
      "0.034594312\n",
      "0.03455664\n",
      "0.034519006\n",
      "0.034481414\n",
      "0.034443863\n",
      "evaluating\n",
      "Got an average of 53.6% accuracy\n",
      "0.034406353\n",
      "evaluating\n",
      "Got an average of 53.6% accuracy\n",
      "0.034368884\n",
      "evaluating\n",
      "Got an average of 53.6% accuracy\n",
      "0.034331456\n",
      "evaluating\n",
      "Got an average of 53.6% accuracy\n",
      "0.034294065\n",
      "evaluating\n",
      "Got an average of 53.6% accuracy\n",
      "0.034256715\n",
      "evaluating\n",
      "Got an average of 53.6% accuracy\n",
      "0.0342194\n",
      "evaluating\n",
      "Got an average of 53.6% accuracy\n",
      "0.03418213\n",
      "evaluating\n",
      "Got an average of 53.6% accuracy\n",
      "0.034144897\n",
      "evaluating\n",
      "Got an average of 53.6% accuracy\n",
      "0.034107707\n",
      "evaluating\n",
      "Got an average of 53.6% accuracy\n",
      "0.03407055\n",
      "evaluating\n",
      "Got an average of 53.6% accuracy\n",
      "0.03403344\n",
      "evaluating\n",
      "Got an average of 53.6% accuracy\n",
      "0.033996366\n",
      "evaluating\n",
      "Got an average of 53.6% accuracy\n",
      "0.033959333\n",
      "evaluating\n",
      "Got an average of 53.6% accuracy\n",
      "0.033922337\n",
      "evaluating\n",
      "Got an average of 53.6% accuracy\n",
      "0.033885382\n",
      "evaluating\n",
      "Got an average of 53.5% accuracy\n",
      "0.03384847\n",
      "evaluating\n",
      "Got an average of 53.6% accuracy\n",
      "0.033811588\n",
      "evaluating\n",
      "Got an average of 53.6% accuracy\n",
      "0.033774752\n",
      "evaluating\n",
      "Got an average of 53.6% accuracy\n",
      "0.03373795\n",
      "evaluating\n",
      "Got an average of 53.6% accuracy\n",
      "0.033701193\n",
      "evaluating\n",
      "Got an average of 53.6% accuracy\n",
      "0.034053314\n",
      "evaluating\n",
      "Got an average of 54.0% accuracy\n",
      "0.03364629\n",
      "evaluating\n",
      "Got an average of 53.9% accuracy\n",
      "0.033627\n",
      "evaluating\n",
      "Got an average of 53.7% accuracy\n",
      "0.033606574\n",
      "evaluating\n",
      "Got an average of 53.6% accuracy\n",
      "0.033585027\n",
      "evaluating\n",
      "Got an average of 53.8% accuracy\n",
      "0.03356238\n",
      "evaluating\n",
      "Got an average of 53.8% accuracy\n",
      "0.03353867\n",
      "evaluating\n",
      "Got an average of 53.7% accuracy\n",
      "0.033513945\n",
      "evaluating\n",
      "Got an average of 53.7% accuracy\n",
      "0.033488266\n",
      "evaluating\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-729184fd01ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"evaluating\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_bongard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconvnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-e71dd3520527>\u001b[0m in \u001b[0;36mtest_bongard\u001b[0;34m(X_val, model)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mn_correct\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mmin_dist\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwhich_is_it\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatabase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mm_val\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mtargets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-f8697fd7cd33>\u001b[0m in \u001b[0;36mwhich_is_it\u001b[0;34m(image, database, model)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mwhich_is_it\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatabase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_to_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mmin_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb_enc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatabase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdb_enc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-6441269f3bae>\u001b[0m in \u001b[0;36mimg_to_encoding\u001b[0;34m(image, model)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mx1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict_on_batch\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1943\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1944\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_predict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1945\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1946\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1947\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2482\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2483\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1137\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1355\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1356\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1359\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[0;32m-> 1340\u001b[0;31m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size=2\n",
    "n_iter=1000\n",
    "y = [i for i in range(batch_size)]\n",
    "qy = np.array(y)\n",
    "\n",
    "print(\"training\")\n",
    "for i in range(1, n_iter):\n",
    "    a,p,n,store= TripletBongard_loader.triplet_generator(batch_size=2)\n",
    "    loss=triplet.train_on_batch([a,p,n], qy)\n",
    "    print(loss)\n",
    "    if i > n_iter/2:\n",
    "        print(\"evaluating\")\n",
    "        val_acc = test_bongard(X_val,convnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
