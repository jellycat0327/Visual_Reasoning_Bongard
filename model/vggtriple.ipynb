{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import Model,load_model\n",
    "from keras.layers import Input, Layer,GlobalAveragePooling2D,Dense\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.regularizers import l2\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import numpy.random as rng\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def W_init(shape,name=None):\n",
    "    \"\"\"Initialize weights as in paper\"\"\"\n",
    "    values = rng.normal(loc=0,scale=1e-2,size=shape)\n",
    "    return K.variable(values,name=name)\n",
    "#//TODO: figure out how to initialize layer biases in keras.\n",
    "def b_init(shape,name=None):\n",
    "    \"\"\"Initialize bias as in paper\"\"\"\n",
    "    values=rng.normal(loc=0.5,scale=1e-2,size=shape)\n",
    "    return K.variable(values,name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"de..., inputs=Tensor(\"in...)`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "input_shape = (105, 105, 3)\n",
    "vgg16_model = VGG16(weights = 'imagenet', include_top = False)\n",
    "x = vgg16_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "predictions = Dense(1024,activation=\"sigmoid\",kernel_regularizer=l2(1e-3),kernel_initializer=W_init,bias_initializer=b_init)(x)\n",
    "convnet=Model(input = vgg16_model.input, output = predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input for anchor, positive and negative images\n",
    "in_a = Input(shape=(105, 105, 3))\n",
    "in_p = Input(shape=(105, 105, 3))\n",
    "in_n = Input(shape=(105, 105, 3))\n",
    "\n",
    "# Output for anchor, positive and negative embedding vectors\n",
    "emb_a = convnet(in_a)\n",
    "emb_p = convnet(in_p)\n",
    "emb_n = convnet(in_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletLossLayer(Layer):\n",
    "    def __init__(self, alpha, **kwargs):\n",
    "        self.alpha = alpha\n",
    "        super(TripletLossLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def triplet_loss(self, inputs):\n",
    "        a, p, n = inputs\n",
    "        p_dist = K.sum(K.square(a-p), axis=-1)\n",
    "        n_dist = K.sum(K.square(a-n), axis=-1)\n",
    "        return K.sum(K.maximum(p_dist - n_dist + self.alpha, 0), axis=0)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        loss = self.triplet_loss(inputs)\n",
    "        self.add_loss(loss)\n",
    "        return loss\n",
    "    \n",
    "# Layer that computes the triplet loss from anchor, positive and negative embedding vectors\n",
    "triplet_loss_layer = TripletLossLayer(alpha=0.2, name='triplet_loss_layer')([emb_a, emb_p, emb_n])\n",
    "\n",
    "# Model that can be trained with anchor, positive negative images\n",
    "triplet_net = Model([in_a, in_p, in_n], triplet_loss_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 105, 105, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 105, 105, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 105, 105, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_1 (Model)                 (None, 4096)         16815936    input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "triplet_loss_layer (TripletLoss [(None, 4096), (None 0           model_1[1][0]                    \n",
      "                                                                 model_1[2][0]                    \n",
      "                                                                 model_1[3][0]                    \n",
      "==================================================================================================\n",
      "Total params: 16,815,936\n",
      "Trainable params: 16,815,936\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "triplet_net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 105, 105, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 105, 105, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 105, 105, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_1 (Model)                 (None, 4096)         16815936    input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "triplet_loss_layer (TripletLoss [(None, 4096), (None 0           model_1[1][0]                    \n",
      "                                                                 model_1[2][0]                    \n",
      "                                                                 model_1[3][0]                    \n",
      "==================================================================================================\n",
      "Total params: 16,815,936\n",
      "Trainable params: 2,101,248\n",
      "Non-trainable params: 14,714,688\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# freeze all layers of the pre-trained model\n",
    "for layer in vgg16_model.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "triplet_net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training alphabets\n",
      "dict_keys(['left', 'right'])\n",
      "validation alphabets:\n",
      "dict_keys(['left', 'right'])\n"
     ]
    }
   ],
   "source": [
    "#load datasets\n",
    "\n",
    "PATH = \"./Bongard/BP_2\" #CHANGE THIS - path where the pickled data is stored\n",
    "\n",
    "with open(os.path.join(PATH, \"train.pickle\"), \"rb\") as f:\n",
    "    (X,c) = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(PATH, \"val.pickle\"), \"rb\") as f:\n",
    "    (X_val,cval) = pickle.load(f)\n",
    "    \n",
    "print(\"training alphabets\")\n",
    "print(c.keys())\n",
    "print(\"validation alphabets:\")\n",
    "print(cval.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data from ./Bongard/BP_2/train.pickle\n",
      "loading data from ./Bongard/BP_2/val.pickle\n"
     ]
    }
   ],
   "source": [
    "class TripletBongard:\n",
    "    def __init__(self, path, data_subsets = [\"train\", \"val\"]):\n",
    "        self.data={}\n",
    "        self.categories={}\n",
    "\n",
    "        for name in data_subsets:\n",
    "            file_path= os.path.join(path, name + \".pickle\")\n",
    "            print(\"loading data from {}\".format(file_path))\n",
    "            with open(file_path,\"rb\") as f:\n",
    "                (X,c) = pickle.load(f)\n",
    "                self.data[name] = X\n",
    "                self.categories[name] = c\n",
    "\n",
    "    def triplet_generator(self, batch_size, s=\"train\"):\n",
    "\n",
    "            X=self.data[s]\n",
    "            n_classes, n_examples, w, h = X.shape\n",
    "\n",
    "            anchor_bongards=np.zeros((batch_size, w, h,3)) \n",
    "            posi_bongards=np.zeros((batch_size,  w, h,3)) \n",
    "            neg_bongards=np.zeros((batch_size,  w, h,3)) \n",
    "            store= targets=np.zeros((batch_size,5))\n",
    "            for i in range(batch_size):\n",
    "\n",
    "                rand_idx = rng.randint(0, n_classes)\n",
    "                anchor_bongard_idx = rng.randint(0, n_examples)\n",
    "                posi_bongard_idx=rng.randint(0, n_examples)\n",
    "\n",
    "                while anchor_bongard_idx == posi_bongard_idx:\n",
    "                    posi_bongard_idx = rng.randint(0, n_examples)\n",
    "\n",
    "\n",
    "                anchor_bongard = cv2.cvtColor(cv2.resize(X[rand_idx, anchor_bongard_idx], (w, h)), cv2.COLOR_GRAY2RGB)\n",
    "                posi_bongard = cv2.cvtColor(cv2.resize(X[rand_idx, posi_bongard_idx], (w, h)), cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "                neg_idx=rng.randint(0, n_classes)\n",
    "                while neg_idx==rand_idx:\n",
    "                    neg_idx = rng.randint(0, n_classes)\n",
    "\n",
    "                neg_bongard_idx=rng.randint(0, n_examples)\n",
    "                neg_bongard = cv2.cvtColor(cv2.resize(X[neg_idx, neg_bongard_idx], (w, h)), cv2.COLOR_GRAY2RGB)\n",
    "                \n",
    " \n",
    "                anchor_bongards[i,:,:,:] = anchor_bongard/255.0\n",
    "                posi_bongards[i,:,:,:] = posi_bongard/255.0\n",
    "                neg_bongards[i,:,:,:] = neg_bongard/255.0\n",
    "               \n",
    "                store[i,0]=rand_idx\n",
    "                store[i,1]= anchor_bongard_idx\n",
    "                store[i,2]= posi_bongard_idx\n",
    "                store[i,3]= neg_bongard_idx\n",
    "                store[i,4]= neg_idx\n",
    "             \n",
    "            return anchor_bongards, posi_bongards,neg_bongards,store\n",
    "        \n",
    "                         \n",
    "#Instantiate the class\n",
    "TripletBongard_loader= TripletBongard(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,p,n,store= TripletBongard_loader.triplet_generator(batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_generator_2( ):\n",
    "    ''' Dummy triplet generator for API usage demo only.\n",
    "    Will be replaced by a version that uses real image data later.\n",
    "    :return: a batch of (anchor, positive, negative) triplets\n",
    "    '''\n",
    "    while True:\n",
    "        a_batch = a\n",
    "        p_batch = p\n",
    "        n_batch = n\n",
    "        yield [a_batch , p_batch, n_batch], None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator2= triplet_generator_2( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:1: UserWarning: Output \"triplet_loss_layer\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"triplet_loss_layer\" during training.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "500/500 [==============================] - 10s 20ms/step - loss: 0.0227\n",
      "Epoch 2/10\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.1917\n",
      "Epoch 3/10\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.0795\n",
      "Epoch 4/10\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.0184\n",
      "Epoch 5/10\n",
      "487/500 [============================>.] - ETA: 0s - loss: 0.0054"
     ]
    }
   ],
   "source": [
    "triplet_net.compile(loss=None, optimizer='adam')\n",
    "triplet_net.fit_generator(generator2, epochs=10, steps_per_epoch=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_to_encoding(image, model):\n",
    "    w, h = image.shape\n",
    "    x=cv2.cvtColor(cv2.resize(image,(w, h)), cv2.COLOR_GRAY2RGB)\n",
    "    x1=x/255.0\n",
    "    x1 = x1[np.newaxis, :]\n",
    "    embedding = model.predict_on_batch(x1)\n",
    "    return embedding   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = {}\n",
    "database[\"left_0\"] = img_to_encoding(X[0][0], convnet)\n",
    "database[\"left_1\"] = img_to_encoding(X[0][1], convnet)\n",
    "database[\"left_2\"] = img_to_encoding(X[0][2], convnet)\n",
    "database[\"left_3\"] = img_to_encoding(X[0][3], convnet)\n",
    "database[\"left_4\"] = img_to_encoding(X[0][4], convnet)\n",
    "database[\"left_5\"] = img_to_encoding(X[0][5], convnet)\n",
    "database[\"right_0\"] = img_to_encoding(X[1][0], convnet)\n",
    "database[\"right_1\"] = img_to_encoding(X[1][1], convnet)\n",
    "database[\"right_2\"] = img_to_encoding(X[1][2], convnet)\n",
    "database[\"right_3\"] = img_to_encoding(X[1][3], convnet)\n",
    "database[\"right_4\"] = img_to_encoding(X[1][4], convnet)\n",
    "database[\"right_5\"] = img_to_encoding(X[1][5], convnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def which_is_it(image, database, model):\n",
    "    encoding = img_to_encoding(image, model)\n",
    "    min_dist = 100\n",
    "    for (name, db_enc) in database.items():\n",
    "        dist = np.linalg.norm(encoding - db_enc)\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            identity = name\n",
    "            \n",
    "    return min_dist, identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_bongard(X_val,model):\n",
    "    n_classes_val, n_examples_val, w, h = X_val.shape\n",
    "    m_val = n_classes_val * n_examples_val\n",
    "    X_val=X_val.reshape(m_val, w, h)\n",
    "    n_correct=0\n",
    "    for i in range(m_val):\n",
    "        min_dist,identity=which_is_it(X_val[i], database, model)\n",
    "        if i < m_val/2:\n",
    "            targets=0\n",
    "        else:\n",
    "            targets=1 \n",
    "        \n",
    "        if identity in [\"left_0\",\"left_1\",\"left_2\",\"left_3\",\"left_4\",\"left_5\"]:\n",
    "            test_result=0\n",
    "        else:\n",
    "            test_result=1\n",
    "            \n",
    "        if test_result== targets:\n",
    "            n_correct+=1\n",
    "         \n",
    "        #print(identity)\n",
    "    percent_correct = (100.0*n_correct / m_val)\n",
    "    print(\"Got an average of {}% accuracy\".format(percent_correct))\n",
    "     \n",
    "    return percent_correct\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_acc = test_bongard(X_val,convnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
